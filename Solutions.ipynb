{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "# Web scraping in Python\n",
    "\n",
    "## Scrapy\n",
    "\n",
    "In this project, you will use Scrapy to scrape content from websites.  Because the scrapy engine is run from the command line, you will develop a spider class and save it to a file, then run the engine using that file.  \n",
    "\n",
    "You can do this entirely within the notebook in the manner shown.  You are welcome to develop the scripts in your favorite editor instead (including Jupyter text editor), but then copy the content into the notebook after it is developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test-spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test-spider.py\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "class PythonTitle(scrapy.Spider):\n",
    "    name = 'Title of python.org'\n",
    "    start_urls = ['https://www.python.org/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        return {'title': BS(response.text).title.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"Welcome to Python.org\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -f test.jl\n",
    "scrapy runspider test-spider.py -o test.jl -L ERROR\n",
    "cat test.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**A fictional bookstore**\n",
    "\n",
    "The URL http://books.toscrape.com/ contains a collection of pages that resemble an online bookstore.  Prices and ratings are randomly assigned by them.  The book titles and authors appear to be actual books, although I have not verified all of them.\n",
    "\n",
    "For this task, we wish to crawl the entire site, and identify every UPC code of every book they stock, and the corresponding URL.  If you perform this scraping correctly, you will identify 1000 such distinct items.  Write your results as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting upc-codes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upc-codes.py\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class UPCSpider(scrapy.Spider):\n",
    "    name = 'UPC codes of all books'\n",
    "    start_urls = ['http://books.toscrape.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # First yield the UPC if it is a book page\n",
    "        for tr in soup.find_all('tr'):\n",
    "            if tr.th.text == 'UPC':\n",
    "                yield {'UPC': tr.td.text, 'URL': response.url}\n",
    "                break\n",
    "                \n",
    "        # Look for links either way\n",
    "        for a in soup.find_all('a'):\n",
    "            href = a['href']\n",
    "            if href.startswith('http') and 'toscrape.com' not in href:\n",
    "                # Only follow domain links\n",
    "                continue\n",
    "            yield response.follow(href, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f upc-codes.csv\n",
    "scrapy runspider upc-codes.py -o upc-codes.csv -L WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UPC</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a18a4f574854aced</td>\n",
       "      <td>http://books.toscrape.com/catalogue/libertaria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb7cc7701ecf901</td>\n",
       "      <td>http://books.toscrape.com/catalogue/olio_984/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e30f54cea9b38190</td>\n",
       "      <td>http://books.toscrape.com/catalogue/mesaerion-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3b1c02bac2a429e6</td>\n",
       "      <td>http://books.toscrape.com/catalogue/scott-pilg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ce6396b0f23f6ecc</td>\n",
       "      <td>http://books.toscrape.com/catalogue/set-me-fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5faefdc861684eb1</td>\n",
       "      <td>http://books.toscrape.com/catalogue/the-sandma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3301af038a720587</td>\n",
       "      <td>http://books.toscrape.com/catalogue/the-comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3cdca3b4a93980f5</td>\n",
       "      <td>http://books.toscrape.com/catalogue/rat-queens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>bcbcbcf0f6ed196f</td>\n",
       "      <td>http://books.toscrape.com/catalogue/paper-girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4f64568dfefa3ce7</td>\n",
       "      <td>http://books.toscrape.com/catalogue/saga-volum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  UPC                                                URL\n",
       "0    a18a4f574854aced  http://books.toscrape.com/catalogue/libertaria...\n",
       "1    feb7cc7701ecf901  http://books.toscrape.com/catalogue/olio_984/i...\n",
       "2    e30f54cea9b38190  http://books.toscrape.com/catalogue/mesaerion-...\n",
       "3    3b1c02bac2a429e6  http://books.toscrape.com/catalogue/scott-pilg...\n",
       "4    ce6396b0f23f6ecc  http://books.toscrape.com/catalogue/set-me-fre...\n",
       "..                ...                                                ...\n",
       "995  5faefdc861684eb1  http://books.toscrape.com/catalogue/the-sandma...\n",
       "996  3301af038a720587  http://books.toscrape.com/catalogue/the-comple...\n",
       "997  3cdca3b4a93980f5  http://books.toscrape.com/catalogue/rat-queens...\n",
       "998  bcbcbcf0f6ed196f  http://books.toscrape.com/catalogue/paper-girl...\n",
       "999  4f64568dfefa3ce7  http://books.toscrape.com/catalogue/saga-volum...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('upc-codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "For a number of years, I wrote programming articles that I republished at my website https://www.gnosis.cx/publish/.  Very often within these articles, I would include a witticism in the last section, titled \"About the Author.\"  All of these articles are \"Web 0.5\" style; very simple like the early web, and without any `class`, `id` or other special attributes on tags.  Sometimes these blurbs are repeated between articles. You should only save one copy of each blurb in the output.\n",
    "\n",
    "For example, one article contained:\n",
    "\n",
    "\n",
    "> About The Author<br/>\n",
    "> David Mertz programs generically and is dispatched multiply. David may be reached at mertz@gnosis.cx; his life pored over at http://gnosis.cx/publish/. Check out David's book Text Processing in Python (http://gnosis.cx/TPiP/).\n",
    "\n",
    "Discarding the portion of the blurb that starts at \"David may be reached at ...\" is probably a good idea.  A good solution will produce a file something like shown.  The order of lines might vary since pages are fetched concurrently.\n",
    "\n",
    "```python\n",
    ">>> for line in open('gnosis-blurbs.csv').readlines()[:7]:\n",
    "...     print(line.strip())\n",
    "```\n",
    "```\n",
    "David Mertz programs generically and is dispatched multiply.\n",
    "David Mertz has a slow brain, and most of his programs still run slowly.\n",
    "David Mertz is feeling a bit testy.\n",
    "David Mertz thinks that artificial languages are perfectly natural, but natural languages seem a bit artificial.\n",
    "David Mertz is blessed with the virtues of laziness, and impatience, and hubris.\n",
    "David Mertz had no idea he was writing prose this whole time.\n",
    "While David Mertz also likes laziness and impatience, this installment is about hubris.\n",
    "```\n",
    "\n",
    "Be careful to limit your scraping to HTML pages under the `/publish/` path.  Otherwise you might scrape a lot that you do not want, and the process may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gnosis-blurbs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gnosis-blurbs.py\n",
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class GnosisSpider(scrapy.Spider):\n",
    "    name = 'Mertz blurbs'\n",
    "    start_urls = ['https://www.gnosis.cx/publish/']\n",
    "    link_extractor = LinkExtractor()\n",
    "    blurbs = set()\n",
    "    \n",
    "    def parse(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Look for the potential \"About\"\n",
    "        for h3 in soup.find_all('h3'):\n",
    "            if h3.text.lower() == 'about the author':\n",
    "                blurb = h3.find_next('p').text\n",
    "                blurb = ' '.join(blurb.split())\n",
    "                blurb = blurb.split(\"David may be reached at\")[0]\n",
    "                if blurb not in self.blurbs:\n",
    "                    self.blurbs.add(blurb)\n",
    "                    yield {'blurb': blurb}\n",
    "                \n",
    "        # Look for links either way\n",
    "        for link in self.link_extractor.extract_links(response):\n",
    "            if 'gnosis.cx/publish' not in link.url or '.htm' not in link.url:\n",
    "                continue    # Only follow links under path\n",
    "            yield response.follow(link, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f gnosis-blurbs.csv\n",
    "scrapy runspider gnosis-blurbs.py -o gnosis-blurbs.csv -L WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

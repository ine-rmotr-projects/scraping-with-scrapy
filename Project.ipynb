{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "# Web scraping in Python\n",
    "\n",
    "## Scrapy\n",
    "\n",
    "In this project, you will use Scrapy to scrape content from websites.  Because the scrapy engine is run from the command line, you will develop a spider class and save it to a file, then run the engine using that file.  \n",
    "\n",
    "You can do this entirely within the notebook in the manner shown.  You are welcome to develop the scripts in your favorite editor instead (including Jupyter text editor), but then copy the content into the notebook after it is developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test-spider.py\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "class PythonTitle(scrapy.Spider):\n",
    "    name = 'Title of python.org'\n",
    "    start_urls = ['https://www.python.org/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        return {'title': BS(response.text).title.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f test.jl\n",
    "scrapy runspider test-spider.py -o test.jl -L ERROR\n",
    "cat test.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**A fictional bookstore**\n",
    "\n",
    "The URL http://books.toscrape.com/ contains a collection of pages that resemble an online bookstore.  Prices and ratings are randomly assigned by them.  The book titles and authors appear to be actual books, although I have not verified all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we wish to crawl the entire site, and identify every UPC code of every book they stock, and the corresponding URL.  If you perform this scraping correctly, you will identify N such distinct codes.  Write your results as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop your crawler as, e.g. `upc-code.py` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !scrapy runspider upc-codes.py  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "For a number of years, I wrote programming articles that I republished at my website https://www.gnosis.cx/publish/.  Very often within these articles, I would include a witticism in the last section, titled \"About the Author.\"  All of these articles are \"Web 0.5\" style; very simple like the early web, and without any `class`, `id` or other special attributes on tags.  Sometimes these blurbs are repeated between articles. You should only save one copy of each blurb in the output.\n",
    "\n",
    "For example, one article contained:\n",
    "\n",
    "\n",
    "> About The Author<br/>\n",
    "> David Mertz programs generically and is dispatched multiply. David may be reached at mertz@gnosis.cx; his life pored over at http://gnosis.cx/publish/. Check out David's book Text Processing in Python (http://gnosis.cx/TPiP/).\n",
    "\n",
    "Discarding the portion of the blurb that starts at \"David may be reached at ...\" is probably a good idea.  A good solution will produce a file something like shown.  The order of lines might vary since pages are fetched concurrently.\n",
    "\n",
    "```python\n",
    ">>> for line in open('gnosis-blurbs.csv').readlines()[:7]:\n",
    "...     print(line.strip())\n",
    "```\n",
    "```\n",
    "David Mertz programs generically and is dispatched multiply.\n",
    "David Mertz has a slow brain, and most of his programs still run slowly.\n",
    "David Mertz is feeling a bit testy.\n",
    "David Mertz thinks that artificial languages are perfectly natural, but natural languages seem a bit artificial.\n",
    "David Mertz is blessed with the virtues of laziness, and impatience, and hubris.\n",
    "David Mertz had no idea he was writing prose this whole time.\n",
    "While David Mertz also likes laziness and impatience, this installment is about hubris.\n",
    "```\n",
    "\n",
    "Be careful to limit your scraping to HTML pages under the `/publish/` path.  Otherwise you might scrape a lot that you do not want, and the process may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
